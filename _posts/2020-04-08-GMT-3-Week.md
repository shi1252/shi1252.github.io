---
layout: post
title:  "GMT 3 Week"
date:   2020-04-22
excerpt: "Game Making Technique 3rd Week Note"
image: ""
---

# Matrix
<hr/>
A rectangle form that has elements listed by row and column.<br>
It is invented to solve Simultaneous linear equation.<br>
\begin{bmatrix}
a & b \\\
c & d \\\
\end{bmatrix}

<hr/>

## Operations
Let
$M =
\begin{bmatrix}
m_{00} & m_{01} \\\
m_{10} & m_{11} \\\
\end{bmatrix}
, \\
N =
\begin{bmatrix}
n_{00} & n_{01} \\\
n_{10} & n_{11} \\\
\end{bmatrix}
$
<hr/>

### Addition
$M + N = 
\begin{bmatrix}
m_{00} & m_{01} \\\
m_{10} & m_{11} \\\
\end{bmatrix}
+
\begin{bmatrix}
n_{00} & n_{01} \\\
n_{10} & n_{11} \\\
\end{bmatrix}
=
\begin{bmatrix}
m_{00} + n_{00} & m_{01} + n_{01} \\\
m_{10} + n_{10} & m_{11} + n_{11} \\\
\end{bmatrix}
$

<hr/>

### Scalar Multiplication
Let scalar $k$<br>
$kM =
k
\begin{bmatrix}
m_{00} & m_{01} \\\
m_{10} & m_{11} \\\
\end{bmatrix}
=
\begin{bmatrix}
km_{00} & km_{01} \\\
km_{10} & km_{11} \\\
\end{bmatrix}
$
<hr/>

### Matrix Multiplication
$MN =
\begin{bmatrix}
m_{00} & m_{01} \\\
m_{10} & m_{11} \\\
\end{bmatrix}
\begin{bmatrix}
n_{00} & n_{01} \\\
n_{10} & n_{11} \\\
\end{bmatrix}
=
\begin{bmatrix}
m_{00}n_{00} + m_{01}n_{10} & m_{00}n_{01} + m_{01}n_{11} \\\
m_{10}n_{00} + m_{11}n_{10} & m_{10}n_{01} + m_{11}n_{11} \\\
\end{bmatrix}
$

<hr/>

### Transpose
$M^T
=
\begin{bmatrix}
m_{00} & m_{10} \\\
m_{01} & m_{11} \\\
\end{bmatrix}
$

<hr/>

## How Linear Transformation Can Be A Matrix
General form of vector space linear transformation.<br>
$f(x, y) = (ax+by,cx+dy) \quad (\mathbb{R}^2 => \mathbb{R}^2)$<br>

Proof.<br>
$f(x, y) = (ax + by, cx + dy), \quad u = (e, f), \quad v = (g, h), \quad k \in \mathbb{R}$<br>
$u + v = (e + g, f + h)$<br>
$f(u + v) = (a \cdot (e + g) + b \cdot (f + h), c \cdot (e + g) + d \cdot (f + h))$<br>
$\begin{align}
f(u) + f(v) & = (a \cdot e + b \cdot f, c \cdot e + d \cdot f) + (a \cdot g + b \cdot h, c \cdot g + d \cdot h) \\\
& = (a \cdot (e + g) + b \cdot (f + h), c \cdot (e + g) + d \cdot (f + h))
\end{align}$<br>
$\therefore f(u + v) = f(u) + f(v)$<br>
$kf(u) = k \cdot (a \cdot e + b \cdot f, c \cdot e + d \cdot f) = (k \cdot (a \cdot e + b \cdot f), k \cdot (c \cdot e + d \cdot f))$<br>
$\begin{align}
f(ku) & = (a \cdot (k \cdot e) + b \cdot (k \cdot f), c \cdot (k \cdot e) + d \cdot (k \cdot f)) \\\
& = (k \cdot (a \cdot e + b \cdot f), k \cdot (c \cdot e + d \cdot f))
\end{align}$<br>
$\therefore kf(u) = f(ku)$

So it can expressed by matrix multiplication.<br>
$
\begin{bmatrix}
a & b \\\
c & d \\\
\end{bmatrix}
\begin{bmatrix}
x \\\
y
\end{bmatrix}
=
\begin{bmatrix}
ax + by \\\
cx + dy
\end{bmatrix}
$

<hr/>

## Why Column Major Matrix Multiplication Is Processed Backward?
Because matrix multiplication doesn't satisfy commutative property.<br>
The target vector of linear transformation in column major matrix is expressed by $
\begin{bmatrix}
x \\\ y
\end{bmatrix}$<br>
So the linear transformation expressed by
$
\begin{bmatrix}
a & b \\\
c & d \\\
\end{bmatrix}
\begin{bmatrix}
x \\\
y
\end{bmatrix}
=
\begin{bmatrix}
ax + by \\\
cx + dy
\end{bmatrix}
$<br>
Let there are linear transform matrix 
$F = 
\begin{bmatrix}
a & b \\\
c & d \\\
\end{bmatrix}
$, 
$G = 
\begin{bmatrix}
e & f \\\
g & h \\\
\end{bmatrix}
$<br>
$\therefore g(f(x,y)) = GF
\begin{bmatrix}
x \\\ y
\end{bmatrix}
=
G(F\begin{bmatrix}
x \\\ y
\end{bmatrix})
$

<hr/>

## Meaning of Column at Column Major Square Matrix
<p>
At linear transforamtion, Each column vector corresponded by each element of target vector means that how the element is going to transform. Which means how the standard basis vector is going to transform.<br>
</p>
Let $A = \begin{bmatrix} a & b \\\ c & d \end{bmatrix}$, $v = \begin{bmatrix} x \\\ y \end{bmatrix}$ which has standard basis vector $i = (1, 0)$, $j = (0, 1)$<br>
$Av = \begin{bmatrix} a & b \\\ c & d \end{bmatrix}\begin{bmatrix} x \\\ y \end{bmatrix} = \begin{bmatrix} ax + by \\\ cx + dy \end{bmatrix}$<br>
$Ai = \begin{bmatrix} a & b \\\ c & d \end{bmatrix}\begin{bmatrix} 1 \\\ 0 \end{bmatrix} = \begin{bmatrix} a \\\ c \end{bmatrix}$<br>
$Aj = \begin{bmatrix} a & b \\\ c & d \end{bmatrix}\begin{bmatrix} 0 \\\ 1 \end{bmatrix} = \begin{bmatrix} b \\\ d \end{bmatrix}$<br>
<br>
So the standard basis vector $i = (1, 0)$ is transformed to $(a, c)$ and $j = (0, 1)$ is transforemd to $(b, d)$.

<hr/>

## Meaning of Determinant at Linear Transformation
<p>If $D \neq 0$ then the linear transform matrix has inverse matrix. Which means the result vector of the linear transformation can return to origin.</p>
<p>If $D = 0$ then the linear transform matrix doesn't have inverse matrix. Which means the result vector of the linear transformation can't return to origin. Because the space of the vector lose dimension. Which means basis vectors making vector space became linearly dependent. So if the space want to return then it has to have same number of basis vector before the transformation. But it doesn't after the transformation because the basis vectors became linearly dependent.</p>

<hr/>

## Why The Dot Product Between Orthogonal Vectors Is 0?
Definition of Dot Product:
<div class = "box">
$X = (x_1, x_2, ..., x_n)$, $Y = (y_1, y_2, ..., y_n)$<br>
Algebraically: $X \cdot Y = x_1y_1 + x_2y_2 + ... + x_ny_n$<br>
Geometrically: $x \cdot y = ||x|| \cdot ||y|| \cdot cos\theta$
</div>
<p><b>Where is the geometrically definition come from?</b><br>
To know that we have to know Law of Cosines.<br>
<p><span class="image img"><img src="{{ "/images/GMT_Week3_Triangle.PNG" | absolute_url }}" alt="" /></span></p>
$a = bcosC + ccosB \quad \because \overline{BC} \perp \overline{OA}$<br>
So we can know next things too by the same logic.<br>
$b = ccosA + acosC$<br>
$c = acosB + bcosA$<br>
And then we can multiply its length to each of them.<br>
$a^2 = abcosC + accosB$<br>
$b^2 = bccosA + abcosB$<br>
$c^2 = accosB + bccosA$<br>
Then we can make equations $a^2 - b^2 - c^2, \quad b^2 - a^2 - c^2, \quad c^2 - a^2 - b^2$<br>
$\begin{align}
a^2 - b^2 - c^2 & = abcosC + accosB - (bccosA + abcosC) - (accosB + bccosA) \\\
& = -2bccosA
\end{align}$<br>
$\therefore a^2 = b^2 + c^2 - 2bccosA$<br>
By the same logic also we can know these.<br>
$b^2 = a^2 + c^2 - 2accosB$<br>
$c^2 = a^2 + b^2 = 2abcosC$<br><br>

And we have to know this vector property.<br>
Let vector $v = (x, y)$<br>
$v \cdot v = x \cdot x + y \cdot y = x^2 + y^2 = ||v||^2$<br><br>

Now we can know how the geometrically definition defined.<br>
<p><span class="image img"><img src="{{ "/images/GMT_Week3_VectorTriangle.png" | absolute_url }}" alt="" /></span></p>
There are vectors $x$, $y$, and angle $\theta$<br>
We know $||x-y||^2$ by the law of cosines.<br>
And also we know $||x-y||^2 = (x-y) \cdot (x-y)$ by the vector property.<br>
$||x-y||^2 = ||x||^2 + ||y||^2 - 2||x||\cdot||y||cos\theta$<br>
$\begin{align}(x-y)\cdot (x-y) & = x \cdot x - x \cdot y - y \cdot x + y \cdot y\\\
& = ||x||^2 + ||y||^2 - 2x \cdot y
\end{align}$<br><br>
Now we can compare both.<br>
$(x-y)\cdot (x-y) = ||x-y||^2$<br>
$||x||^2 + ||y||^2 - 2x \cdot y = ||x||^2 + ||y||^2 - 2||x||\cdot||y||cos\theta$<br>
$-2x \cdot y = -2||x||\cdot||y||cos\theta$<br>
$\therefore x\cdot y = ||x||\cdot||y||cos\theta$<br>

</p>
Let there are orthogonal vectors. Then the angle between vectors is $90^\circ$.<br>
So by the geometrically definition $x \cdot y = ||x|| \cdot ||y|| \cdot cos\theta = ||x|| \cdot ||y|| \cdot cos90^\circ = 0$.<br>
So the dot product between orthogonal vectors is $0$.

<hr/>

## Why Orthogonal Matrix's Inverse Matrix Is Same With Its Transpose Matrix?
<p>Orthogonal Matrix is composed of orthonormal vectors. Which means each column of orthogonal matrix is orthonormal vector.</p>
<p>Orhtonormal vector is a vector that length is $1$ and perpendicular with other vector pair.</p>
<p>
Let there are orthonormal vectors $u = \begin{bmatrix} x \\\ y \end{bmatrix}$, $v = \begin{bmatrix} a \\\ b \end{bmatrix}$<br>
$u^T \cdot u = \begin{bmatrix} x & y \end{bmatrix} \begin{bmatrix} x \\\ y \end{bmatrix} = x^2 + y^2 = 1 \quad (\because ||u|| = \sqrt {x^2 + y^2} = 1)$.<br>
$u^T \cdot v = \begin{bmatrix} x & y \end{bmatrix} \begin{bmatrix} a \\\ b \end{bmatrix} = xa + yb = 0\quad (\because u \perp v)$<br>
So let orthogonal matrix $M = \begin{bmatrix} x & a \\\ y & b \end{bmatrix} \quad M^T = \begin{bmatrix} x & y \\\ a & b \end{bmatrix}$<br>
$M^TM = \begin{bmatrix} x & y \\\ a & b \end{bmatrix} \begin{bmatrix} x & a \\\ y & b \end{bmatrix} = \begin{bmatrix} x^2 + y^2 & xa + yb \\\ xa + yb & a^2 + b^2 \end{bmatrix} = \begin{bmatrix} 1 & 0 \\\ 0 & 1 \end{bmatrix} = I$<br>
$\therefore M^T = M^{-1}$</p>

<hr/>