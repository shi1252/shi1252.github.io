---
layout: post
title:  "GMT 3 Week"
date:   2020-04-22
excerpt: "Game Making Technique 3rd Week Note"
image: ""
---

# Matrix
<hr/>
A rectangle form that has elements listed by row and column.<br>
It is invented to solve Simultaneous linear equation.<br>
\begin{bmatrix}
a & b \\\
c & d \\\
\end{bmatrix}

<hr/>

## Operations
Let
$M =
\begin{bmatrix}
m_{00} & m_{01} \\\
m_{10} & m_{11} \\\
\end{bmatrix}
, \\
N =
\begin{bmatrix}
n_{00} & n_{01} \\\
n_{10} & n_{11} \\\
\end{bmatrix}
$
<hr/>

### Addition
$M + N = 
\begin{bmatrix}
m_{00} & m_{01} \\\
m_{10} & m_{11} \\\
\end{bmatrix}
+
\begin{bmatrix}
n_{00} & n_{01} \\\
n_{10} & n_{11} \\\
\end{bmatrix}
=
\begin{bmatrix}
m_{00} + n_{00} & m_{01} + n_{01} \\\
m_{10} + n_{10} & m_{11} + n_{11} \\\
\end{bmatrix}
$

<hr/>

### Scalar Multiplication
Let scalar $k$<br>
$kM =
k
\begin{bmatrix}
m_{00} & m_{01} \\\
m_{10} & m_{11} \\\
\end{bmatrix}
=
\begin{bmatrix}
km_{00} & km_{01} \\\
km_{10} & km_{11} \\\
\end{bmatrix}
$
<hr/>

### Matrix Multiplication
$MN =
\begin{bmatrix}
m_{00} & m_{01} \\\
m_{10} & m_{11} \\\
\end{bmatrix}
\begin{bmatrix}
n_{00} & n_{01} \\\
n_{10} & n_{11} \\\
\end{bmatrix}
=
\begin{bmatrix}
m_{00}n_{00} + m_{01}n_{10} & m_{00}n_{01} + m_{01}n_{11} \\\
m_{10}n_{00} + m_{11}n_{10} & m_{10}n_{01} + m_{11}n_{11} \\\
\end{bmatrix}
$

<hr/>

### Transpose
$M^T
=
\begin{bmatrix}
m_{00} & m_{10} \\\
m_{01} & m_{11} \\\
\end{bmatrix}
$

<hr/>

## How Linear Transformation Can Be A Matrix
General form of vector linear transformation.<br>
$f(x, y) = (ax+by,cx+dy)$<br><br>
So it can expressed by matrix multiplication.<br>
$
\begin{bmatrix}
a & b \\\
c & d \\\
\end{bmatrix}
\begin{bmatrix}
x \\\
y
\end{bmatrix}
=
\begin{bmatrix}
ax + by \\\
cx + dy
\end{bmatrix}
$

<hr/>

## Why Column Major Matrix Multiplication Is Processed Backward?
Because matrix multiplication doesn't satisfy commutative property.<br>
The target vector of linear transformation in column major matrix is expressed by $
\begin{bmatrix}
x \\\ y
\end{bmatrix}$<br>
So the linear transformation expressed by
$
\begin{bmatrix}
a & b \\\
c & d \\\
\end{bmatrix}
\begin{bmatrix}
x \\\
y
\end{bmatrix}
=
\begin{bmatrix}
ax + by \\\
cx + dy
\end{bmatrix}
$<br>
Let there are linear transform matrix 
$F = 
\begin{bmatrix}
a & b \\\
c & d \\\
\end{bmatrix}
$, 
$G = 
\begin{bmatrix}
e & f \\\
g & h \\\
\end{bmatrix}
$<br>
$\therefore g(f(x,y)) = GF
\begin{bmatrix}
x \\\ y
\end{bmatrix}
=
G(F\begin{bmatrix}
x \\\ y
\end{bmatrix})
$

<hr/>

## Meaning of Column at Column Major Square Matrix
<p>
At linear transforamtion, Each column vector corresponded by each element of target vector means that how the element is going to transform. Which means how the standard basis vector is going to transform.<br>
</p>
Let $A = \begin{bmatrix} a & b \\\ c & d \end{bmatrix}$, $v = \begin{bmatrix} x \\\ y \end{bmatrix}$ which has standard basis vector $i = (1, 0)$, $j = (0, 1)$<br>
$Av = \begin{bmatrix} a & b \\\ c & d \end{bmatrix}\begin{bmatrix} x \\\ y \end{bmatrix} = \begin{bmatrix} ax + by \\\ cx + dy \end{bmatrix}$<br>
$Ai = \begin{bmatrix} a & b \\\ c & d \end{bmatrix}\begin{bmatrix} 1 \\\ 0 \end{bmatrix} = \begin{bmatrix} a \\\ c \end{bmatrix}$<br>
$Aj = \begin{bmatrix} a & b \\\ c & d \end{bmatrix}\begin{bmatrix} 0 \\\ 1 \end{bmatrix} = \begin{bmatrix} b \\\ d \end{bmatrix}$<br>
<br>
So the standard basis vector $i = (1, 0)$ is transformed to $(a, c)$ and $j = (0, 1)$ is transforemd to $(b, d)$.

<hr/>

## Meaning of Determinant at Linear Transformation
<p>If $D \neq 0$ then the linear transform matrix has inverse matrix. Which means the result vector of the linear transformation can return to origin.</p>
<p>If $D = 0$ then the linear transform matrix doesn't have inverse matrix. Which means the result vector of the linear transformation can't return to origin. Because the space of the vector lose dimension. Which means basis vectors making vector space became linearly dependent. So if the space want to return then it has to have same number of basis vector before the transformation. But it doesn't after the transformation because the basis vectors became linearly dependent.</p>

<hr/>

## Why The Dot Product Between Orthogonal Vectors Is 0?
Definition of Dot Product:
<div class = "box">
$X = (x_1, x_2, ..., x_n)$, $Y = (y_1, y_2, ..., y_n)$<br>
Algebraically: $X \cdot Y = x_1y_1 + x_2y_2 + ... + x_ny_n$<br>
Geometrically: $x \cdot y = ||x|| \cdot ||y|| \cdot cos\theta$
</div>
Let there are orthogonal vectors. Then the angle between vectors is $90^\circ$.<br>
So by the geometrically definition $x \cdot y = ||x|| \cdot ||y|| \cdot cos\theta = ||x|| \cdot ||y|| \cdot cos90^\circ = 0$.<br>
So the dot product between orthogonal vectors is $0$.

<hr/>

## Why Orthogonal Matrix's Inverse Matrix Is Same With Its Transpose Matrix?
<p>Orthogonal Matrix is composed of orthonormal vectors. Which means each column of orthogonal matrix is orthonormal vector.</p>
<p>Orhtonormal vector is a vector that length is $1$ and perpendicular with other vector pair.</p>
<p>
Let there are orthonormal vectors $u = \begin{bmatrix} x \\\ y \end{bmatrix}$, $v = \begin{bmatrix} a \\\ b \end{bmatrix}$<br>
$u^T \cdot u = \begin{bmatrix} x & y \end{bmatrix} \begin{bmatrix} x \\\ y \end{bmatrix} = x^2 + y^2 = 1 \quad (\because ||u|| = \sqrt {x^2 + y^2} = 1)$.<br>
$u^T \cdot v = \begin{bmatrix} x & y \end{bmatrix} \begin{bmatrix} a \\\ b \end{bmatrix} = xa + yb = 0\quad (\because u \perp v)$<br>
So let orthogonal matrix $M = \begin{bmatrix} x & a \\\ y & b \end{bmatrix} \quad M^T = \begin{bmatrix} x & y \\\ a & b \end{bmatrix}$<br>
$M^TM = \begin{bmatrix} x & y \\\ a & b \end{bmatrix} \begin{bmatrix} x & a \\\ y & b \end{bmatrix} = \begin{bmatrix} x^2 + y^2 & xa + yb \\\ xa + yb & a^2 + b^2 \end{bmatrix} = \begin{bmatrix} 1 & 0 \\\ 0 & 1 \end{bmatrix} = I$<br>
$\therefore M^T = M^{-1}$</p>

<hr/>